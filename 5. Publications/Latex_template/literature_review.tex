\section{Text Embedding}
Text embedding methods can be roughly classified by the level of encoding unit (embed in a word, sentence, or article level), supervised or unsupervised, similarity measurement (such as Euclidean distance or cosine similarity) and other specific technique support such as term-based embedding in particular fields and graph-based embedding.
Some advanced methods such as ELMo\cite{peters2018} and GPT\cite{brown2020} use autoregressive models and BERT\cite{niven2019} uses an autoencoder to do bidirectional context encoding.
A more complex structure of neural networks and a bigger training corpus could derive a better and more robust model but a huge amount of time to train models as well.
Hence, another improvement of embedding methods would be the trade-off between better performance and reasonable training time.

\subsubsection{Bag of Word \& TF-IDF}
Bag-Of-Word (BOW) is one of the approaches to implement the vector space model\cite{salton1975} in the NLP field and one of the most straightforward ways to extract information from texts.
BOW uses word counts as the representation of texts, not taking the order, grammar and semantic meaning of words into account.
A fixed-length vector is used to record the appearance frequency of a word, encoding a word into a sparse vector.\cite{sethy2008}
BOW can successfully categorise texts\cite{zhang2010}, while text analysis from other aspects is still possible to be explored.

Term frequency-inverse document frequency (TF-IDF) model uses the word occurrence in sentences and documents to denote the importance of words.
TF-IDF assumes a word to play an important role if it occurs frequently only in some sentences as the key idea of this document.
An important word will have a higher term frequency and a lower document frequency, which makes the TF-IDF higher.
On the opposite, a less important word equally occurs in most of the sentences, generally used in many situations and topics but implying fewer ideas.\cite{li2011}

\subsubsection{Latent Semantic Analysis}
Latent Semantic Analysis (LSA), also known as Latent Semantic Indexing (LSI), is a technique in NLP and information retrieval first used in 1990 \cite{deerwester1990}.
LSA applies math techniques, such as singular value decomposition (SVD, $M=U \Sigma V$), to a term-document matrix, such as TF-IDF matrix, to discover hidden patterns in texts and convert the matrix to lower dimensions.
LSA decomposes a term-document matrix ($M$) into a term-topic matrix ($U$), a topic-document matrix ($V^T$) and a topic-topic diagonal matrix ($\Sigma$) denotes the importance of a certain topic in the collection.
LSA is an unsupervised and relatively simple model compared to those neuron-network methods, making it widely used for identifying latent topics, evaluating document similarity and information retrieval.

\subsubsection{Word2Vec: Continuous Bag of Word and Skip Gram}
Word2Vec is and neuron network-based approach to perform word embedding, introduced by Google in 2013\cite{mikolov2013efficient}\cite{mikolov2013distributed}.
Word2vec encodes the word by the context of the target word and aims to maximise the probability of predicting the context word(s) as the object of the NN model.
Word2vec can be implemented in two ways: continuous bag of words model (CBOW) and skip-gram model (SG), which are both 3-layer NN models but require different input and prediction.
The 3-layer NN structure includes an input layer, a hidden layer and an output layer along with a softmax function to yield the output probabilities for every unique word in the document to present in the context given the target word.
CBOW uses the context-before and context-after to predict the central word, while SG uses the central word to predict its context-before and context-after.
Word2vec is a self-supervised algorithm, which means the NN model needs the central word for CBOW and the context words for SG as the expectation of training, while the prediction labels are implied in the content hence we don't have to provide them to the model.

The training of NN model starts from the one-hot embedding of target words as an input vector.
The non-linear activation function, usually in the hidden and output layers, is removed to provide a more efficient calculation.
The output vector is the size of unique words and is usually very long, making it a huge time-consuming to calculate the denominator of softmax function.
Hence, \cite{mikolov2013efficient} described hierarchical softmax to speed up the process with a binary tree, the Huffman tree, and reduce the time complexity from O(n) to O(log n).
Additionally, within a certain window size, only a few words are present in the context of the target word compared to the number of all unique words, which means the expected output vector of target words is highly sparse.
The negative sampling had been presented by \cite{mnih2012} \cite{gutmann2012} and was involved to select less number of not-presented candidates (negative samples) to do the softmax with presented candidates (positive samples), described in \cite{mikolov2013distributed} as an alternative to hierarchical softmax.

\subsubsection{Global Vectors for Word Representation}
Global Vectors for Word Representation (GloVe)\cite{pennington2014} is a co-occurrence matrix-based method, calculated with the sliding window just like the window size of word2vec.
The co-occurrence matrix records the frequency of a pair of words present in the same context of a fixed length of words.
This matrix is then transformed into a ratio of two probabilities: (1)the co-occurrence probability of a candidate given one target term, and (2)the co-occurrence probability of the same candidate given another target term.
For a given candidate word, the ratio matrix denotes a larger value if the numerator term co-occurs with the candidate word more often than the denominator term does, which indicates a more similar semantic of the numerator to the candidate word, and vice versa.
If the candidate words have similar relations to both two target words, the probability ratio would be close to 1.

Compare to Bag-of-Word and LSA, instead of using a whole sentence or paragraph, GloVe uses a sliding window to count the frequency of co-occurrence, which brings the distance information of the pair of words to the co-occurrence matrix.
Also, compare to word2vec, GloVe choose a non-NN method to keep the structure fast and simple.
GloVe and word2vec both use a sliding window to extract context information, but word2vec uses context words and target words as input and target output to train the parameters in the model and GloVe uses context to calculate co-occurrence.

\subsubsection{FastText}
FastText\cite{bojanowski2016} is famous for its application in document classification tasks, however, it can also be used to embed words.
FastText applies character n-grams features to the target word of word2vec model to not only embed a whole word but also subwords, making it possible to deal with out-of-vocab words, rare words and misspellings.
Additionally, FastText learns information from subwords including roots and affixes, which usually contains critical information about whole word semantics.
Subwords are generated by breaking down the target word n-characters frame, initially embedded and summed up as the input vector of the word2vec model (CBOW or SG) for the target word.
Once comes an unseen word, FastText would split it into subwords and look up the training subwords for embedding vectors.
The embedding vectors of existing subwords would be summed up as the embedding vector of the unseen word.

\subsubsection{Recurrent Neural Network}
Recurrent Neural Network (RNN) is to deal with time series data as an input of NN, which contains time-ordered relation in a series of elements and requires a proper structure to handle the relation within elements.
Natural language can be treated as time-series data since the meaning of a sentence can sometimes highly depend on the order of words and the meaning of a word can also depend on the context before and after\cite{mikolov2011extensions}.
In a hidden layer, a contextual vector called hidden state is introduced as a memory to transmit information from the former timestep to the latter timestep.
The output hidden state at time timestep t $h'(t)$ is generated by input hidden state $h(t) = h'(t-1)$, neuron input at this timestep $x(t)$, two weights $W_x, W_h$ and a bias $b_h$, while the neuron output at this timestep $y(t)$ is linearly transformed from output hidden state $h'(t)$, a weight $b_y$ and a bias $b_y$.
The information of the former timesteps is involved and considered in the calculation of the current timestep and the current decision can depend on previous data, making good use of time-dependence data.
However, the outputs at every timestep have to be calculated one by one, along with the transmission of the hidden layer state, making it difficult to parallelly compute a series of data.

Considering the fact that basic RNN can only pass a hidden state from the former timestep to a latter timestep, while in some situations the information afterwards can also highly relate to the decision-making currently.
Hence, the bidirectional recurrent neural networks (BRNN)\cite{schuster1997bidirectional} is performed as a two-way RNN to be involved in the computation at the current timestep.
There are two loops and two sets of hidden states in BRNN to transfer information to the previous timestep and the following timestep respectively to provide information before and after.

\subsubsection{Long-Short Term Memory}
To take the information at further timesteps into account, Long-Short Term Memory (LSTM) introduced cell state to store and transfer long-term memory.
Cell state ($c(t), c'(t)$) works like a conveyor belt, puts important memory into records and forgets less-important information along the timesteps.
In a hidden layer, the hidden state vector would be concatenated with the input vector.
Forget gate, input gate and output gate would then be formed via the concatenated vector, three weights ($W_f$, $W_i$, and $W_o$, separately) and a sigmoid transformation.
The concatenated vector is also transformed by the tanh function and multiplied by the input gate to form a new information vector to store in long-term memory.
As for the cell state, it is first multiplied by the forget gate to control how much the information would remain in long-term memory, then added by the new information vector to pass to the next timestep.
As for the hidden state, the processed cell state is utilised to be transformed by the tanh function and multiplied by the output gate to pass to the next timestep.
Finally, as for the output of the current timestep, the processed hidden state is utilised to be multiplied by a weight vector and transformed by the sigmoid function.

Like RNN, LSTM can also be built bidirectionally as biLSTM\cite{graves2013hybrid}.
LSTM is widely used in NLP\cite{wang-nyberg-2015-long}, time series analysis\cite{selvin2017stock}, image captioning\cite{vinyals2015show}, healthcare\cite{rajkomar2018scalable}, etc.
Especially, LSTM excels in NLP tasks including language modeling\cite{sundermeyer2012lstm}, sentiment analysis\cite{wang2016attention} and text generation\cite{guo2018long} due to the benefit of dealing with sequence input and taking into account the information far from current word.
However, like RNN, it takes more effort and advanced tricks for the implementation of LSTM to be parallelly computed\cite{yangyang2021research}.

\subsubsection{Embeddings from Language Model}
Embeddings from Language Model (ELMo)\cite{peters2018} is one of the most popular applications of LSTM in NLP tasks.
ELMo consists of $L$ bidirectional LSTM (biLM) models and yields $L+1$ latent representations of word embedding, one initial input vector and $L$ hidden states vectors $h'$ from each biLM.
The output of ELMo is generated by weighting and summing up the $L+1$ latent representations.
In the original paper, ELMo obtains initial input vectors from a pretrained character-level word embedding model cnn-big-lstm\cite{jozefowicz2016exploring} developed by Google.
cnn-big-lstm is trained by a character-level CNN to receive character tokens, a base LSTM model and several small LSTM models to predict target words char-by-char, which enables ELMo to understand the semantics of roots, affixes and other latent meanings.
ELMo is famous in NLP tasks including sentiment analysis, named entity recognition, dependency parsing\cite{dozat2016deep}, and conference resolution\cite{lee2018higher}.

\subsubsection{self-attention}
Self-attention is another approach, developed by Google, to deal with time series data and aims to consider neighbour timesteps while calculating the current timestep but can be parallelly computed\cite{vaswani2017attention}.
Self-attention calculates the relation between the query element (current word) and key elements (context words) with three weights ($W_q, W_k, W_v$) instead of passing hidden state and cell state in RNN or LSTM.
When currently inputting the $t^{th}$ word, The query word vector ($x(t)$) and key word vectors ($x(i), i\neq t $) would be multiplied by $W_q$, $W_k$ respectively to generate weighted query vector ($q(t)$) and weighted key vectors ($k(i), i\neq t$), and then the dot product between weighted query vector ($q(t)$) and each of the weighted key vectors ($k(i), i\neq t$) would be computed as the relation between the target word and context words ($\omega (i), i\neq t $).
The third weight, $W_v$, multiplies every input value to generate weighted input vectors ($v(i)$), which are then multiplied by the corresponding normalised relation ($\omega '(i), i\neq t $) and summed up to yield the output of hidden layer($y(t)$).

Although it's not technically a bidirectional structure, taking the pairwise relation into account enables self-attention to contextually embed words.
Additionally, without passing hidden states and cell states, self-attention can be implemented parallelly.
Like RNN and LSTM, the number of neighbour words to pay attention to can be adjusted by passing arguments.
In image processing, Convolutional Neural Network (CNN) can also be seen as a special case of self-attention with appropriate parameters\cite{cordonnier2019relationship}.

Multi-head self-attention is a widely used extension of self-attention, which splits the weighted query vectors, key vectors and input vectors into several streams, computes relations within the same stream, and concatenates the outputs from different streams.
Multi-head attention allows the model to focus on different features from different perspectives within subspaces.
Since self-attention estimates the relation of words regardless of the location or distances of words, positional encoding is used to provide position information of series elements to the model, by manually adding a specific matrix such as one-hot or sin function to the input data series to denote the order of serial input vectors.

\subsubsection{transformer}
Transformer uses self-attention layers to achieve sequence-to-sequence tasks contextually, consisting of two parts: encoder and decoder.
Encoder deals with sequence input by self-attention layers, requires a set of initial embedding vectors of words from input sentences, and yields a set of vectors sharing the same length as the input sentence.
Encoder's output would then be applied to the second self-attention layer in the decoder. 
Decoder manages to generate sequence output based on the understanding of the whole input sequence, while the output sequence doesn't have to correspond to the input sequence and might have a different length from the input sequence.

When training a transformer, the decoder has an input sequence as a prediction label and is processed by the first self-attention layer to embed the prediction label words.
In the second self-attention layer in decoder, instead of computing relations between each word in the prediction label, they would be treated as query words and the output vectors from encoder would be imported as key words.
The relations of word vectors would be calculated between prediction label vectors and input embedding vectors from the encoder parallelly, hence transformers can deal with seq2seq tasks regardless of the word count of neither input nor output sentence.
Decoder's output is a set of prediction vectors representing the probability of the corresponding word, and the objective function of a transformer is to maximise the probabilities.
However, when generating a sentence from an unknown input sentence, no prediction label is provided.
The input and output of encoder remain the same, and the input of decoder would be a special token, <START>, representing the starting point of the output sentence.
The output sentence would be generated word-by-word hence not in parallel.
Especially, the first self-attention layer in decoder is called masked self-attention since a former word wouldn't be able to compute the relation to a latter word to mimic a real-word seq2seq situation.

\subsubsection{Universal Sentence Encoder}
Universal Sentence Encoder (USE) is an NN-based end-to-end sentence embedding technique to encode a sentence to a 512-length vector.
Instead of generating the sentence embedding vector by taking an average of all of the word vectors in the sentence, USE provides two ways in its original paper to embed the whole sentence.
The first approach stacks six transformer encoders to form the encoder of USE, while a transformer encoder consists of a self-attention and a feed forward network.
This transformer-based model has the benefit of better accuracy in later tasks such as sentence classification, however, the complicated structure of stacked transformers and several self-attention modules results in the time complexity of $O(n^2)$ in sentence length and consumes more computing time and resources.
The second approach utilises Deep Averaging Network (DAN) to avoid the huge computation of self-attention.
DAN takes every single word and the word bi-grams as tokens, takes the average of all tokens, and applies four feed forward layers to generate the sentence embedding vector.
The word bi-grams serves contextual information to the embedding model but reduces the computation of self-attention models, however, DAN-based model yields a lower accuracy compared to the transformation-based model.

\subsubsection{BERT}
Sample text sample text sample text sample text sample text

\subsubsection{Applications of Text Embedding Techniques}
Text embedding techniques are improved for specific objectives, fields and styles and applied to a wide range of tasks, including various issues of tweets analysis\cite{mottaghinia2021}, visualisation in the biomedical field\cite{oubenali2022} and sentiment analysis on movie reviews\cite{sivakumar2021}.

Khatua et al.\cite{khatua2019} identified crisis-related tweets during the 2014 Ebola and 2016 Zika outbreaks with pre-trained Word2Vec and GloVe models.
They found a better classification performance to have a small domain-specific corpus from tweets and scholarly abstracts from PubMed participated in the model.
They also observed a higher accuracy from a higher dimension of word vector and skip-gram model than CBOW.

Lee et al.\cite{lee2017} utilized SentiWordNet 3.0 to analyse the effect of several negative emotions in hotel reviews.
SentiWordNet 3.0\cite{baccianella2010} provided sentiment analysis as classification tasks and word embedding with a frequency-weighted bag-of-words model and the help of WordNet corpus.
Onan\cite{onan2021} presented a sentiment analysis approach to product reviews from Twitter.
This deep-learning-based method applied TF-IDF weighted GloVe to the CNN-LSTM architecture to do word embedding and outperformed conventional deep-learning methods.


\section{Text Similarity}
A pair of similar words should act similarly in most of the features we extracted, while a good similarity metric can aggregate the difference in every feature into a single value, which is comparable between each pair of words.
Besides Euclidean distance and cosine similarity as semantic similarity measurements mentioned above, WordNet also provides path similarity to evaluate similarities of words with a lexical hierarchical structure.

Sentence similarity measurements are more complex and various compared to word similarity since sentences can be considered as combinations of words.
The most straightforward approach is to aggregate words in the sentence as a representation to compare with other sentences, which can be seen as a baseline measurement of sentence similarity.
The steps are to take averages of every word in each sentence, yield a single vector for each sentence and calculate the Euclidean distance or cosine similarity between two sentences with average vectors.
This approach, however, doesn't consider the order of words, while it can have a huge effect on a sentence's meaning when the word order differs.

Several algorithms are proposed to map a sentence into a vector and calculate the similarity with the derived value directly from the embedding process.
Much of them are extended from an existing word embedding method to apply to sentences or even documents, such as Word2Vec, Sent2Vec and Doc2Vec.
Some Approaches consider different levels of embedding at the same time, such as word embedding and sentence embedding of BERT.

\subsubsection{Lexical Relation and WordNet}
Sample text sample text sample text sample text sample text


\section{Text Embedding for Search Engine}
With the increase of documents and web pages, traditional keywords-based search engines are thought to be powerless to correctly look for users' requirements.
Text embedding techniques are applied to search engines to provide machine-readable web pages and semantic annotations to the algorithm to yield a more accurate search result.
Many websites are applying text embedding to their search engines, including Google Search, Microsoft's Bing Search, Amazon Search and many e-commerce websites and platforms.
Not to confuse the search engine using text embedding and the search engine using semantic web search language, in this paper, we refer to the semantic search as the searching approach with text embedding or other semantic retrieval techniques. 

Regarding the search engines for scientific articles, Eisenberg et al.\cite{eisenberg2017} proposed a semantic search for Biogeochemical literature that undergoes paper research by comparing the concepts extracted from queries and academic literature.
However, the concept extraction component in the workflow of this research is annotated by domain experts, which can be done automatically by NLP approaches mentioned in the future directions chapter.
Fang et al.\cite{fang2018} performed a biomedical article-searching approach called Semantic Sequential Dependence Model (SSDM) that combined semantic information retrieval techniques and the traditional SDM model.
Word embedding techniques of the Neural Network Language Model(NNLM)\cite{bengio2000}, Log-Bilinear Language Model(LBL)\cite{mnih2007} and Word2vec are applied to the literature corpus to find synonyms by KNN classification algorithm and generate a domain-specific thesaurus.
The thesaurus is then utilized to extend query keywords and the SDM played an important role in the combination strategy of the extended keywords for further comparison to documents.

Compare to the keyword embedding approaches, applying sentence embedding or other wider-level embedding techniques to search engines can better preserve information for query but be more challenging at the same time.
Palangi et al.\cite{palangi2016} managed to embed semantic information at a sentence level by using LSTM-RNN (Long Short-Term Memory - Recurrent Neural Network) framework to do web document retrieval, and compare the summarisation sentence vector and query sentence vector to yield the best searching result.


\section{Searching-Based Citation Recommendation}

Some academic databases come with citation recommendation tools, which provide relevant articles that share the same category with or are similar to our research.
Citation recommendation tools would yield different results resulting from not only different algorithms they used but also candidate papers' published journals, impact factors, times being referred to, and the availability if it's open to everyone or subscription-only.

Citation recommendation methods are always built with three main stages: (1) Generate candidate citations from the publication database (2) Create a recommendation list by ranking the candidate citations (3) Evaluate the accuracy of our recommendation system.\cite{ma2020}
Text embedding techniques can be included generally in step 1: generation of candidate citation, that is, applying text embedding to filter out candidate citations from the publication database that better relate to keywords provided by users or meet users' needs.

Since the diversity of terms and patterns between scientific articles in different domains, researchers would tend to train a specific model with their domain data or use transfer learning to fine-tune the model parameters.
Tshitoyan et al.\cite{tshitoyan2019} used modified Word2Vec embedding to successfully capture complex materials science concepts, without any additional chemistry knowledge insertion, and extract knowledge and relationship from scientific literature.
Zhang et al.\cite{zhang2022} used 15 text representation models, including 6 term-based methods, 2 word embedding methods, 3 sentence embedding methods, 2 document embedding methods and 2 BERT-based methods, to construct an article recommendation system in the biomedical field.
They found BERT and BioSenVec, a Sent2Vec model trained on PubMed corpus, outperformed most of the online and offline citation recommendation systems and an improvement in BERT-based methods after fine-tuning to learn users' preferences.

Wang et al.\cite{wang2022} proposed a sentence-level citation recommendation system called SenCite that used CNN to recognise candidate citation sentences and FastText as the word embedding method to extract information from texts, without summarising an article into sentences.
They evaluated the performance of SenCite with several evaluation metrics such as modified reciprocal rank, average precision and normalized discounted cumulative gain and human experts verification.
The SenCite is shown to outperform most of the embedding methods and yield a comparable accuracy to BERT.
The human experts also stated that SenCite provided the best top-1 citation recommendation.