\section{Text Embedding}
Text embedding methods can be roughly classified by the level of encoding unit (embed in a word, sentence, or article level), supervised or unsupervised, similarity measurement (such as Euclidean distance or cosine similarity) and other specific technique support such as term-based embedding in particular fields and graph-based embedding.
Some advanced methods such as ELMo\cite{peters2018} and GPT\cite{brown2020} use autoregressive models and BERT\cite{niven2019} uses an autoencoder to do bidirectional context encoding.
A more complex structure of neural networks and a bigger training corpus could derive a better and more robust model but a huge amount of time to train models as well.
Hence, another improvement of embedding methods would be the trade-off between better performance and reasonable training time.

\subsubsection{Bag of Word \& TF-IDF}
Bag-Of-Word (BOW) is one of the approaches to implement the vector space model\cite{salton1975} in the NLP field and one of the most straightforward ways to extract information from texts.
BOW uses word counts as the representation of texts, not taking the order, grammar and semantic meaning of words into account.
A fixed-length vector is used to record the appearance frequency of a word, encoding a word into a sparse vector.\cite{sethy2008}
BOW can successfully categorise texts\cite{zhang2010}, while text analysis from other aspects is still possible to be explored.

Term frequency-inverse document frequency (TF-IDF) model uses the word occurrence in sentences and documents to denote the importance of words.
TF-IDF assumes a word to play an important role if it occurs frequently only in some sentences as the key idea of this document.
An important word will have a higher term frequency and a lower document frequency, which makes the TF-IDF higher.
On the opposite, a less important word equally occurs in most of the sentences, generally used in many situations and topics but implying fewer ideas.\cite{li2011}

\subsubsection{Latent Semantic Analysis}
Latent Semantic Analysis (LSA), also known as Latent Semantic Indexing (LSI), is a technique in NLP and information retrieval first used in 1990 \cite{deerwester1990}.
LSA applies math techniques, such as singular value decomposition (SVD, $M=U \Sigma V$), to a term-document matrix, such as TF-IDF matrix, to discover hidden patterns in texts and convert the matrix to lower dimensions.
LSA decomposes a term-document matrix ($M$) into a term-topic matrix ($U$), a topic-document matrix ($V^T$) and a topic-topic diagonal matrix ($\Sigma$) denotes the importance of a certain topic in the collection.
LSA is an unsupervised and relatively simple model compared to those neuron-network methods, making it widely used for identifying latent topics, evaluating document similarity and information retrieval.

\subsubsection{Word2Vec: Continuous Bag of Word and Skip Gram}
Word2Vec is and neuron network-based approach to perform word embedding, introduced by Google in 2013\cite{mikolov2013efficient}\cite{mikolov2013distributed}.
Word2vec encodes the word by the context of the target word and aims to maximise the probability of predicting the context word(s) as the object of the NN model.
Word2vec can be implemented in two ways: continuous bag of words model (CBOW) and skip-gram model (SG), which are both 3-layer NN models but require different input and prediction.
The 3-layer NN structure includes an input layer, a hidden layer and an output layer along with a softmax function to yield the output probabilities for every unique word in the document to present in the context given the target word.
CBOW uses the context-before and context-after to predict the central word, while SG uses the central word to predict its context-before and context-after.
Word2vec is a self-supervised algorithm, which means the NN model needs the central word for CBOW and the context words for SG as the expectation of training, while the prediction labels are implied in the content hence we don't have to provide them to the model.

The training of NN model starts from the one-hot embedding of target words as an input vector.
The non-linear activation function, usually in the hidden and output layers, is removed to provide a more efficient calculation.
The output vector is the size of unique words and is usually very long, making it a huge time-consuming to calculate the denominator of softmax function.
Hence, \cite{mikolov2013efficient} described hierarchical softmax to speed up the process with a binary tree, the Huffman tree, and reduce the time complexity from O(n) to O(log n).
Additionally, within a certain window size, only a few words are present in the context of the target word compared to the number of all unique words, which means the expected output vector of target words is highly sparse.
The negative sampling had been presented by \cite{mnih2012} \cite{gutmann2012} and was involved to select less number of not-presented candidates (negative samples) to do the softmax with presented candidates (positive samples), described in \cite{mikolov2013distributed} as an alternative to hierarchical softmax.

\subsubsection{Global Vectors for Word Representation}
Global Vectors for Word Representation (GloVe) is a co-occurrence matrix-based method, calculated with the sliding window just like the window_size of word2vec.
The co-occurrence matrix records the frequency of a pair of words present in the same context of a fixed length of words.
This matrix is then transformed into a ratio of two probabilities: (1)the co-occurrence probability of a candidate given one target term, and (2)the co-occurrence probability of the same candidate given another target term.
For a given candidate word, the ratio matrix denotes a larger value if the numerator term co-occurs with the candidate word more often than the denominator term does, which indicates a more similar semantic of the numerator to the candidate word, and vice versa.
If the candidate words have similar relations to both two target words, the probability ratio would be close to 1.

Compare to Bag-of-Word and LSA, instead of using a whole sentence or paragraph, GloVe uses a sliding window to count the frequency of co-occurrence, which brings the distance information of the pair of words to the co-occurrence matrix.
Also, compare to word2vec, GloVe choose a non-NN method to keep the structure fast and simple.
GloVe and word2vec both use a sliding window to extract context information, but word2vec uses context words and target words as input and target output to train the parameters in the model and GloVe uses context to calculate co-occurrence.

\subsubsection{FastText}
FastText is famous for its application in document classification tasks, however, we can still use it to embed words.
FastText applies character n-grams features to word2vec model to not only embed a whole word but also subwords, making it possible to deal with out-of-vocab words.


\subsubsection{RNN}
Sample text sample text sample text sample text sample text

\subsubsection{Long-Short Term Memory}
Sample text sample text sample text sample text sample text

\subsubsection{ELMo}
Sample text sample text sample text sample text sample text

\subsubsection{self-attention}
Sample text sample text sample text sample text sample text

\subsubsection{transformer}
Sample text sample text sample text sample text sample text

\subsubsection{Universal Sentence Encoder}
Sample text sample text sample text sample text sample text

\subsubsection{BERT}
Sample text sample text sample text sample text sample text

\subsubsection{GPT}
Sample text sample text sample text sample text sample text

\subsubsection{Applications of Text Embedding Techniques}
Text embedding techniques are improved for specific objectives, fields and styles and applied to a wide range of tasks, including various issues of tweets analysis\cite{mottaghinia2021}, visualisation in the biomedical field\cite{oubenali2022} and sentiment analysis on movie reviews\cite{sivakumar2021}.

Khatua et al.\cite{khatua2019} identified crisis-related tweets during the 2014 Ebola and 2016 Zika outbreaks with pre-trained Word2Vec and GloVe models.
They found a better classification performance to have a small domain-specific corpus from tweets and scholarly abstracts from PubMed participated in the model.
They also observed a higher accuracy from a higher dimension of word vector and skip-gram model than CBOW.

Lee et al.\cite{lee2017} utilized SentiWordNet 3.0 to analyse the effect of several negative emotions in hotel reviews.
SentiWordNet 3.0\cite{baccianella2010} provided sentiment analysis as classification tasks and word embedding with a frequency-weighted bag-of-words model and the help of WordNet corpus.
Onan\cite{onan2021} presented a sentiment analysis approach to product reviews from Twitter.
This deep-learning-based method applied TF-IDF weighted GloVe to the CNN-LSTM architecture to do word embedding and outperformed conventional deep-learning methods.


\section{Text Similarity}
A pair of similar words should act similarly in most of the features we extracted, while a good similarity metric can aggregate the difference in every feature into a single value, which is comparable between each pair of words.
Besides Euclidean distance and cosine similarity as semantic similarity measurements mentioned above, WordNet also provides path similarity to evaluate similarities of words with a lexical hierarchical structure.

Sentence similarity measurements are more complex and various compared to word similarity since sentences can be considered as combinations of words.
The most straightforward approach is to aggregate words in the sentence as a representation to compare with other sentences, which can be seen as a baseline measurement of sentence similarity.
The steps are to take averages of every word in each sentence, yield a single vector for each sentence and calculate the Euclidean distance or cosine similarity between two sentences with average vectors.
This approach, however, doesn't consider the order of words, while it can have a huge effect on a sentence's meaning when the word order differs.

Several algorithms are proposed to map a sentence into a vector and calculate the similarity with the derived value directly from the embedding process.
Much of them are extended from an existing word embedding method to apply to sentences or even documents, such as Word2Vec, Sent2Vec and Doc2Vec.
Some Approaches consider different levels of embedding at the same time, such as word embedding and sentence embedding of BERT.

\subsubsection{Lexical Relation and WordNet}
Sample text sample text sample text sample text sample text


\section{Text Embedding for Search Engine}
With the increase of documents and web pages, traditional keywords-based search engines are thought to be powerless to correctly look for users' requirements.
Text embedding techniques are applied to search engines to provide machine-readable web pages and semantic annotations to the algorithm to yield a more accurate search result.
Many websites are applying text embedding to their search engines, including Google Search, Microsoft's Bing Search, Amazon Search and many e-commerce websites and platforms.
Not to confuse the search engine using text embedding and the search engine using semantic web search language, in this paper, we refer to the semantic search as the searching approach with text embedding or other semantic retrieval techniques. 

Regarding the search engines for scientific articles, Eisenberg et al.\cite{eisenberg2017} proposed a semantic search for Biogeochemical literature that undergoes paper research by comparing the concepts extracted from queries and academic literature.
However, the concept extraction component in the workflow of this research is annotated by domain experts, which can be done automatically by NLP approaches mentioned in the future directions chapter.
Fang et al.\cite{fang2018} performed a biomedical article-searching approach called Semantic Sequential Dependence Model (SSDM) that combined semantic information retrieval techniques and the traditional SDM model.
Word embedding techniques of the Neural Network Language Model(NNLM)\cite{bengio2000}, Log-Bilinear Language Model(LBL)\cite{mnih2007} and Word2vec are applied to the literature corpus to find synonyms by KNN classification algorithm and generate a domain-specific thesaurus.
The thesaurus is then utilized to extend query keywords and the SDM played an important role in the combination strategy of the extended keywords for further comparison to documents.

Compare to the keyword embedding approaches, applying sentence embedding or other wider-level embedding techniques to search engines can better preserve information for query but be more challenging at the same time.
Palangi et al.\cite{palangi2016} managed to embed semantic information at a sentence level by using LSTM-RNN (Long Short-Term Memory - Recurrent Neural Network) framework to do web document retrieval, and compare the summarisation sentence vector and query sentence vector to yield the best searching result.


\section{Searching-Based Citation Recommendation}

Some academic databases come with citation recommendation tools, which provide relevant articles that share the same category with or are similar to our research.
Citation recommendation tools would yield different results resulting from not only different algorithms they used but also candidate papers' published journals, impact factors, times being referred to, and the availability if it's open to everyone or subscription-only.

Citation recommendation methods are always built with three main stages: (1) Generate candidate citations from the publication database (2) Create a recommendation list by ranking the candidate citations (3) Evaluate the accuracy of our recommendation system.\cite{ma2020}
Text embedding techniques can be included generally in step 1: generation of candidate citation, that is, applying text embedding to filter out candidate citations from the publication database that better relate to keywords provided by users or meet users' needs.

Since the diversity of terms and patterns between scientific articles in different domains, researchers would tend to train a specific model with their domain data or use transfer learning to fine-tune the model parameters.
Tshitoyan et al.\cite{tshitoyan2019} used modified Word2Vec embedding to successfully capture complex materials science concepts, without any additional chemistry knowledge insertion, and extract knowledge and relationship from scientific literature.
Zhang et al.\cite{zhang2022} used 15 text representation models, including 6 term-based methods, 2 word embedding methods, 3 sentence embedding methods, 2 document embedding methods and 2 BERT-based methods, to construct an article recommendation system in the biomedical field.
They found BERT and BioSenVec, a Sent2Vec model trained on PubMed corpus, outperformed most of the online and offline citation recommendation systems and an improvement in BERT-based methods after fine-tuning to learn users' preferences.

Wang et al.\cite{wang2022} proposed a sentence-level citation recommendation system called SenCite that used CNN to recognise candidate citation sentences and FastText as the word embedding method to extract information from texts, without summarising an article into sentences.
They evaluated the performance of SenCite with several evaluation metrics such as modified reciprocal rank, average precision and normalized discounted cumulative gain and human experts verification.
The SenCite is shown to outperform most of the embedding methods and yield a comparable accuracy to BERT.
The human experts also stated that SenCite provided the best top-1 citation recommendation.