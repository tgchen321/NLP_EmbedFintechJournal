\section{Data Collection}
We use the Web of Science search engine and database to find the fintech articles and their journals.
Reference papers were collected by the WOS search engine with rules listed as follows:
\begin{itemize}
    \item Topic: "\textbf{Fintech}" or "\textbf{Financial technology}" or "\textbf{digital finance}" or "\textbf{electronic banking}" or "\textbf{cryptocurrency}" or "\textbf{Blockchain}"
    \item Document Types: \textbf{Article} or \textbf{Proceeding Paper} or \textbf{Review Article}
    \item Languages: English
\end{itemize}

We got 66,974 results on 23rd July 2023 from the Web of Science Core Collection and the top 10 publications with the most article are as follow:

\begin{table}
\caption{Table 1. The top 10 publications with the most fintech articles.}
\centering
    \begin{tabular}{lc}
    Journal                                      & \# of papers \\ \hline
    IEEE ACCESS                                  & 1342         \\
    SUSTAINABILITY                               & 1207         \\
    SENSORS                                      & 505          \\
    IEEE INTERNET OF THINGS JOURNAL              & 489          \\
    JOURNAL OF CLEANER PRODUCTION                & 415          \\
    APPLIED SCIENCES-BASEL                       & 404          \\
    ENVIRONMENTAL SCIENCE AND POLLUTION RESEARCH & 377          \\
    ENERGIES                                     & 374          \\
    ELECTRONICS                                  & 344          \\
    TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE  & 326         
    \end{tabular}
    \end{table}

Due to the time restriction mentioned in the following section, we utilise the fintech articles from a single Journal \textit{IEEE Access} as the raw data of this research.
The fintech articles are exported to \texttt{EndNote 20}\footnote{https://endnote.com/} and the \texttt{Find Full Text} feature is used to look for their full text by their Accession numbers including Digital Object Identifier (DOI), WOS ID and PubMed ID.
This feature enables us to automatically batch-download loads of reference papers as PDF files if the journal is available in Open Access resources.

\section{Data Structuring}
We convert the unstructured PDF file into a structured database consisting of pairs of a citation sentence and its full-text reference paper.
Firstly, we extract the main content with citations and the bibliography of the articles from PDF files to a plain text file and a dictionary, respectfully.
This process is to remove non-organised and less-informed texts, such as page numbers, article titles, journal names, images, math equations and paragraph/section labels, to make sure the sentences in the main content can be fully extracted for further analysis.
The bibliography of the articles is also extracted and the information of each reference paper is well-organised to a dictionary.
The reference papers extracted from an article are serialised by \texttt{pickle}\footnote{https://docs.python.org/3/library/pickle.html} module to a list of dictionaries.
The citation sentence can then look up to its corresponding reference paper via the information in this reference dictionary and be matched with the full text.
The Python tool \texttt{PyMuPDF}\footnote{https://pypi.org/project/PyMuPDF/} is used to obtain the style and location of lines, spans, and sections in a PDF page.
The patterns including font size, style and colour are set manually and utilised to recognise if the texts in the current span are the main content, bibliography, or non of both.
Hence, the design and layout of a journal would highly affect the criteria of the extraction.
We would have to customise the process of data extraction from one journal to another, which takes a lot of time and effort and we're not yet able to program the automatic recognition.

Secondly, The sentences that are inspired by reference papers, called citation sentences, are extracted and organised with the reference number.
We split the full texts into sentences and recognise the citation sentences by square brackets with a number in them.
The citation sentence and reference number would be organised and serialised into a list.
Last but not least, the citation sentence and the reference paper are matched together to form the database for this research.
The reference information of the citation sentence can be found via the reference number and further match the citation sentence to the plain text of the reference paper.
The citation sentences from all of the articles and their reference texts are saved as a list of dictionaries.
The whole dataset is used for unsupervised models, while a 7:3 ratio is applied to split the database into the training set and the test set for supervised models.

\section{Data Preprocessing}
Data preprocessing includes every approach to convert the database to the required input of every embedding model.
Among all the preprocessing steps, \texttt{unidecode} is priorly applied to texts to convert special characters to the closest English characters, such as \textit{\o}, \textit{\^o}, \textit{\~o}, \textit{\=o}, \textit{\'o}, \textit{\u o}, \textit{\v o} and ligature such as \textit{\ae}, \textit{\oe}, \textit{f\i}, \textit{fl} and \textit{ff}.
Sentence tokenisation is applied to separate the whole article into sentences basically by the period mark.
Word tokenisation is applied to sentences that split them into words roughly by space.
Stop words are removed from the word tokens and lemmatisation is applied to the word tokens to reduce inflected forms of a word into the word's lemma to treat them as a single word since they contain a similar meaning even if they look different.
The above process is implemented by Natural Language Toolkit, \texttt{nltk}.

\section{Experiments}
In this research, we design two modules in the workflow to make good use of different kinds of embedding methods, the vectorisation module and the similarity module.
The vectorisation module aims to embed a series of words, which can be a sentence or an article, in a word level or sentence level by corresponding embedding algorithms to a single vector.
When a word embedding approach is applied to the texts, several word vectors are generated depending on the word count of the texts, and a single embedding vector for the texts would be the average of several word vectors.
While a sentence embedding approach is applied, a single embedding vector is generated directly.

The similarity between the citation sentence and its reference article is evaluated as the performance of the embedding method.
The similarity module aims to generate a single similarity score for each citation sentence that can reflect either citation sentence-reference sentences similarities or citation sentence-reference article similarity.
In the vectorisation module, a reference article can be vectorised sentence-by-sentence, yielding several sentence vectors depending on the number of sentence in the article, or document-by-document, yielding only one embedding vector.
When the article is vectorised at a sentence level, several vector similarities are computed by comparing several sentence vectors to the citation sentence and the average of top-10 vector similarities is generated as the single similarity score of the citation sentence.
While the article is vectorised at an article level, a single similarity score is generated by comparing between citation sentence vector and reference article vector.

We utilise 8 text embedding algorithms, details listed as follows:

\subsubsection{1. TF-IDF}
The \texttt{TfidfModel} from \texttt{gensim} package is used as an unsupervised sentence embedding approach in this research.
All of the citation sentences and reference sentences are input together to generate the bag-of-word matrix for further calculation of TF-IDF.

\subsubsection{2. LSA}
The \texttt{TruncatedSVD} from \texttt{sklearn} package is used to perform singular vector decomposition on a TF-IDF matrix.
The \texttt{explained\_variance\_ratio\_} is utilised to decide the number of topics for LSA model.
The Bag-Of-Word and TF-IDF matrixes are generated identically in the last section.

\subsubsection{3. Word2Vec}
The \texttt{Word2Vec} from \texttt{gensim} package is used to train our financial-specific academic embedding model, serving as a word-embedding approach.
All of the citation sentences and reference sentences are input together to CBOW and skip-gram methods.
The minimum word count is set to 1 to retain every unique word in our data.
The vector size is set to 100 to embed a word into a 100*1 vector.
The window size is set to 5 to consider the central word with the context of 5 words before and 5 words after.

\subsubsection{4. GloVe}
A PyTorch-based package \texttt{TorchText} is used to perform the GloVe as a word embedding algorithm with pre-trained word vectors \texttt{glove\.6B}, 100-dimension version.
The 6B word vector is trained on \textit{Wikipedia} 2014 corpus and \textit{English Gigaword Fifth Edition}\cite{Gigaword5}, within 6 billion tokens and 400 billion vocabularies.

\subsubsection{5. FastText}
The \texttt{train\_unsupervised} function from \texttt{FastText} library\footnote{https://fasttext.cc/docs/en/python-module.html}, presented by \textit{Facebook Open Source}, is used to train our financial-specific academic embedding model subword-supporting as a word embedding approach.
The parameters detail and input data for both CBOW and skip-gram methods are identical to the previous section \textit{3. Word2Vec}.
In this study, we would focus on the word representation tasks, which is the \texttt{train\_unsupervised}, instead of the text classification part.

\subsubsection{6. ELMo}
A TensorFlow-based library \texttt{simple-elmo} is used to perform word embedding by the pre-trained English ELMo model trained on Wikipedia Dump of October 2019 in a vector size of 1024.
The \texttt{simple-elmo} implementation is slightly different from the original ELMo paper but the main algorithm remains unchanged and easy to use.

\subsubsection{7. USE}
A pre-trained, DAN-based text embedding model from TensorFlow Hub\footnote{https://tfhub.dev/google/universal-sentence-encoder/4} is served as a sentence embedding approach provided by \textit{Google}.
This \texttt{universal-sentence-encoder} is an encoder of greater-than-word length text, applying to sentences, phrases and paragraphs, requires a flexible length of English texts and generates a 512-length vector.

\subsubsection{8. BERT}
We introduce a base pre-trained BERT model and two fine-tuned variants of BERT models provided by \textit{Hugging Face} to perform word embedding on sentences.
We choose a case-insensitive base BERT model \texttt{bert-base-uncased}, imported via \texttt{transformer} package, to obtain the embedding word vectors

The first variant of BERT model we use is \texttt{msmarco-distilbert-base-tas-b} transformer, which includes 6 layers of DistilBert and is trained by \textit{Microsoft Machine Reading Comprehension (MS MARCO)}.
The DistilBert\cite{Hofstaetter2021_tasb_dense_retrieval} is used to reduce the calculation and memory cost, having 40\% fewer parameters and running 60\% faster but keeping 95\% of performance.
The \textit{MS MARCO} is a large dataset that pays attention to question-answering and passage-ranking tasks, along with the balanced topic-aware sampling (TAS-B) included in the transformer.
The \texttt{msmarco-distilbert-base-tas-b} embeds texts into 768 dimension vectors and is optimised for semantic search tasks.

The other BERT variant used in this research is \texttt{all-MiniLM-L6-v2}, which is a 6-layer, case-insensitive distilled unified language model (UniLM)\cite{bao2020unilmv2} that can be fine-tuned for natural language understanding (NLU) and natural language generation (NLG) tasks.
It is fine-tuned by a data collection with over 1 billion sentences and embeds sentences into 384-dimension vectors.
Both of the BERT variants are applied to our financial data via API requests and serve as sentence embedding approaches.
